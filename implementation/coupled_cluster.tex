\chapter{Coupled Cluster}

The main product of this study is manifested in the \lstinline{coupled_cluster}
module for Python. This module is designed to fit together with the
\lstinline{quantum_systems} module described in the previous chapter. We have tried to 
make this module easy to extend, resulting in a framework where every solver scheme 
inherits from an abstract parent class that specifies what must be implemented in order 
to make a supplemental solver or class operational in conjunction with the rest of the 
framework.

As a beginning to this project, which we hope will continue to grow and be used, 
we have implemented several different ground state solver classes, and several
time-dependent solver classes. In order of increasing sophistication and 
elegance, we have a ground state- and a time-dependent solver for both the coupled cluster
method
with double excitations (CCD), the coupled cluster method with singles- and double 
excitations (CCSD), and for the orbital-adaptive coupled cluster method with double
excitations (OACCD). The time-dependent solvers within a particular categor are 
dependent on its ground state counterpart, but the ground state solvers can be used
independently.

The \lstinline{coupled_cluster} module can be install from github via \lstinline{pip}
by the following command,
\begin{lstlisting}[language=bash]
pip install git+https://github.com/Schoyen/coupled-cluster.git
\end{lstlisting}
If one prefers, the same task can be accomplished by the following commands,
\begin{lstlisting}[language=bash]
git clone https://github.com/Schoyen/coupled-cluster.git
cd coupled-cluster
pip install .
\end{lstlisting}
We have supplied environment specifications for \lstinline{conda}, with requirement 
specifications for the convenience of the user. Assuming the git repository is cloned 
properly,
\begin{lstlisting}[language=bash]
conda environment create -f environment.yml
\end{lstlisting}
Activate the environment with,
\begin{lstlisting}[language=bash]
conda activate cc
\end{lstlisting}
Full documentation of this module, which we hope will be kept up to date with any future revisions 
can be found at \url{www.coupled-cluster.com}.

\section{Ground State Computations}

    Before any development in time can be performed, we need to derives systems that 
    we can be pretty certain exist in nature. This makes the implementation of ground 
    state solvers necessary. We have implemented ground state solvers; the
    \lstinline{CoupledClusterDoubles} and \lstinline{CoupledClusterSinglesDoubles} 
    are based on the theoretical framework form the Lagrangian formulation of coupled 
    cluster, while \lstinline{OATDCCD} is a ground state version of an orbital-adaptive 
    ground state coupled cluster solver with double excitations. Moreover, we constructed 
    a data structure for the ampltudes in the \lstinline{AmplitudeContainer} class and 
    we have implemented two mixers that helt with convergences of the ground state solvers,
    \lstinline{AlphaMixer} and \lstinline{DIIS}.

    \subsection{Representation of Amplitudes}

    The most central structure in any coupled cluster solver are the amplitudes. The 
    amplitudes are what defines the true structure of the wavefunction as a linear 
    combination of single-particle functions contained in the reference Slater 
    determinant. We have found it beneficial to implement a special container 
    class for the amplitudes, aptly called \lstinline{AmplitudeContainer}.

    \input{implementation/doc/amplitude_container.tex}

    The \lstinline{AmplitudeContainer} class is built as a data structure for 
    the amplitude functions, and comprises all methods and attributes to serve this 
    purpose. This includes includes overloading of primitive methods of the base 
    python \lstinline{Object} type.: \lstinline{__add__} and \lstinline{__radd__}
    enables adding a scalar or properly shaped vector to the amplitudes, 
    \lstinline{__mul__} and \lstinline{__rmul__} allows for multiplication 
    with scalars and vectors, and \lstinline{__iter__} is implemented to make 
    the class an iterable. In summary, the \lstinline{AmplitudeContainer} 
    functions as a fully functional data structure for amplitudes of coupled 
    cluster solver, with both $\tau$ and $\lambda$ amplitudes.

    \subsection{Coupled Cluster Base Class}

    All ground state solvers within the \lstinline{coupled_cluster} module are built 
    as sub-classes of the abstract base class \lstinline{CoupledCluter}. The most
    important method of this class is the \lstinline{compute_ground_state()} method.
    This method in turn calls the \lstinline{iterate_t_amplitudes()} and 
    \lstinline{iterate_l_amplitudes()} successively. 

    \input{implementation/doc/coupled_cluster.tex}

    As we have outlined in 
    \autoref{ch:coupled_cluster_theory}, the $\tau$ amplitudes are only dependent on 
    $\tau$, while the $\lambda$ amplitudes are dependent on both $\tau$ and $\lambda$.
    Therefore, the $\tau$ amplitude equation iterative solver
    \lstinline{iterate_t_amplitudes()} is called first, and the $\lambda$ amplitude
    equation solver is called second.
    For illustration, the most important section of the \lstinline{compute_l_amplitudes()} method 
    is the following
    \begin{python}
    for i in range(max_iterations):
    self.compute_l_amplitudes()
    residuals = self.compute_l_residuals()

    if self.verbose:
        print(f"Iteration: {i}\tResiduals (l): {residuals}")

    if all(res < tol for res in residuals):
        break

    assert i < (
        max_iterations - 1
    ), f"The l amplitudes did not converge. Last residual: {residuals}" 
    \end{python}
    The equivalent section of code in the \lstinline{compute_t_amplitudes()} method is 
    nearly identical.
    The \lstinline{CoupledCluster} class is supposed to provide a framework for which 
    to implement various coupled cluster ground state solver classes. It therefore
    has several abstract methods that such subclasses need to implement and overwrite.
    The most important of these are the methods \lstinline{compute_t_amplitudes} 
    and \lstinline{compute_l_amplitudes}, which are supposed to contain the evaluation 
    of amplitude equations for a given coupled cluster truncation and scheme. 

    With the hope that the functunality of the rest of the methods 
    in the abstract base class \lstinline{CoupledCluster} can be inferred from 
    name, and with the goal of brevity we proceed to a study of the simplest 
    ground state coupled cluster solver, namely CCD, implemented in the 
    \lstinline{CoupledClusterDoubles} class. 

    \subsection{Coupled Cluster Doubles}
    
    Starting from construction, the \lstinline{CoupledClusterDoubles} class passes 
    the system, defined through a \lstinline{QuantumSystem} object to the 
    parent class constructor, along with any keyword arguments, such as turning 
    on verbosity, mixer type to us and what matrix library to apply. The
    \lstinline{QuantumSystem} class will contain all the information necessary to 
    set up the system, i.e. construct a one-body matrix, fock matrix and two-body 
    matrix. These will be used to set up empty arrays for the $\tau$ and $\lambda$ 
    amplitudes. The \lstinline{comput_initial_guess} is called lastly in the 
    constructor, computing the inital guess of the double-excited amplitudes as 
    \begin{equation}
        \label{eq:ccd_inital_guess}
        \tau^{(0)} = \frac{u^{ab}_{ij}}{D^{ab}_{ij}},
    \end{equation}
    where $u$ is the two-body operator and
    $D^{ab}_{ij} = f^a_a + f^b_b - f^i_i - f^j_j$,
    where $f$ is the Fock operator.
    
    In the \lstinline{CoupledClusterDobles} class specification one would
    notice that it has implementations of all the abstract methods 
    from the \lstinline{CoupledCluster} abstract class. The reason for the existence 
    of the class, the \lstinline{compute_ground_state()} method, is inherited from the 
    parent class, and does the same thing as described above - calling 
    \lstinline{iterate_t_amplitudes()} and \lstinline{iterate_l_amplitudes()}. These 
    methods alos excist as members of \lstinline{CoupledClusterDoubles}, but are excluded 
    from the class specification for sake of brevity. It is 
    possible to pass arguments to the the two iterator methods; one list for each iteration
    method, or as keywords.
    One can also pass arguments 
    to the mixer through the \lstinline{compute_ground_state_method()}. 
    An overview of mixing applied to iterative solvers is given in the next 
    section.

    The important part of the specific coupled cluster scheme solver is contained in the two 
    methods \lstinline{compute_t_amplitudes()} and \lstinline{compute_l_amplitudes()}.
    These functions evaluate the entire coupled cluster doubles amplitude equations.
    The computation of each term (diagram) in the amplitude equation is done in separate functions,
    as calls to \lstinline{numpy.tensordot()}, for a total of ten terms for the 
    $\tau$ amplitude equation in the coupled cluster doubles method including 
    permutation operators:
    \begin{equation}
        \label{eq:ccd_tau}
        \begin{aligned}
        0 =& u^{ab}_{ij} + f^b_c \tau^{ac}_{ij}P(ab) 
            - f^k_j \tau^{ab}_{ik}P(ij)
            + \frac{1}{4} \tau^{ac}_{ij} \tau^{ab}_{mn}u^{mn}_{cd} 
            + \frac{1}{2} \tau^{cd}_{ij}u^{ab}_{cd}
            + \frac{1}{2} \tau^{cd}_{jm} \tau^{ab}_{in} u^{mn}_{cd}P(ij) \\
            &\ - \frac{1}{2} \tau^{ac}_{nm} \tau^{bd}_{ij} u^{nm}_{cd}P(ab)
            + \tau^{ac}_{im} \tau^{bd}_{jn}u^{mn}_{cd}P(ij)
            + \tau^{ac}_{im}u^{bm}_{jc}P(ab)P(ij)
            + \frac{1}{2} \tau^{ab}_{mn}u^{mn}_{ij}.
        \end{aligned}
    \end{equation}

    \input{implementation/doc/ccd.tex}
   
    THIS IS NOT CORRECT WHEN INCLUDING LAMBDA..
    The initial guess in equation \autoref{eq:ccd_inital_guess} is terms 2 and 3
    from \autoref{eq:ccd_tau}. These terms also form the basis of the iterative scheme,
    if we move them to the left of the equal sign in \autoref{eq:ccd_tau}, 
    \begin{equation}
        D^{ab}_{ij} \tau^{ab}_{ij} = g(u, \tau),
    \end{equation}
    where $g(u, \tau)$ now consists of the rest of the doubles amplitude equation, our 
    recursion relation can be written
    \begin{equation}
        t^{(k+1)} = \frac{g(u,\tau^{(k)})}{D^{ab}_{ij}}.
    \end{equation}
    
    An example of a computation of one term from \autoref{eq:ccd_tau} is,
    \begin{python}
    def add_d2e_t(u, t, o, v, out, np):
        term = np.tensordot(t, u[o, v, v, o], axes=((1, 3), (2, 0))).transpose(
            0, 2, 1, 3
        )
        term -= term.swapaxes(0, 1)
        term -= term.swapaxes(2, 3)
        out += term
    \end{python}
    This function particular computes the $D_{2e}$ diagram\footnote{After the labelling from 
    \autoref{ch:coupled_cluster_theory} and Shavitt \& Bartlett\cite{shavitt2009many}}.

    \subsection{Coupled Cluster Singles Doubles}

    Most of the rest of the methods in the \lstinline{CoupledClusterDoubles} class are there 
    for the use of other methods, or for extracting observables. Moving to the next logical 
    coupled cluster solver scheme; the coupled cluster method with single- and double 
    excitations is now a matter of taking into account the extra computations needed in 
    this scheme, for each method in the abstract base clase \lstinline{CoupledCluster}. 
    There are indeed many more computations, but the code will structurally be the same. 
    The class specification for \lstinline{CoupledClusterSinglesDoubles} is therefore 
    given here without specification of the methods as they are excactly the same. For testing 
    purposes, the \lstinline{CoupledClusterSingelsDoubles} class have the possibility 
    to only include double excitation at construction.

    \input{implementation/doc/ccsd.tex}

    \subsection{Orbital-Adaptive Coupled Cluster}

    The algorithm applied when computing the ground state in the orbital-adaptive sphere 
    is the nonorthogonal orbital-optimised coupled cluster (NOCC) method, developed by 
    Myhre\cite{myhre2018demonstrating}. The NOCC scheme is shown to converge towards full
    configuration interaction. Since the \lstinline{OACCD} class is acutally applying 
    NOCC it can be perceived as a misnomer, but as of yet there exist no ground state 
    equivalent of the time-dependent 
    orbital-adaptive coupled cluster (OACC) method. Such a method is in development, and there
    is strong indication that NOCC would be equivalent to a OACC ground state solver. What is 
    more, NOCC does vary the orbitals as well as iterate over amplitude, and we have therefore 
    opted to call it OACC.

    \input{implementation/doc/oaccd.tex}

    Our implementation of the NOCC ground state solver is inderited from code written by Rolf Myhre and 
    adapted to our 
    framework. We supply a brief overview of the algorithm here. The starting point for the 
    NOCC model is the bivariational Lagrangian
    \begin{equation}
        \label{eq:nocc_lagrangian}
        \mathscr{L} = \mel*{\tilde{\Psi}}{\hat{H}}{\Psi} \\
            = \mel*{\tilde{\phi}}
                {
                (1 + \Lambda) e^{-\hat{T}}e^{-\kappa}\hat{H}e^{\kappa}e^{\hat{T}}
                }{\phi}
    \end{equation}
    which is very similar to the coupled cluster Lagrangian (\autoref{eq:cc_energy_lagrangian}),
    except for a biorthogonal basis and a transformation of the Hamiltonian, defined 
    as follows
    \begin{equation}
        \begin{aligned}
            \tilde{c}_p^\dagger &= e^{-\kappa}\hat{c}_p^\dagger e^\kappa \\
            c_p &= e^{-\kappa} \hat{c}_p^\dagger e^\kappa \\
            \ket{\phi} &= e^{-\kappa}\ket*{\hat{\phi}}
        \end{aligned}
    \end{equation}
    where the orthogonal reference creation- and annihilation operators marked with a hat
    ($\hat{\ }$), as is the reference state function. We require that $\kappa$ is antihermitian,
    \begin{equation}
        \kappa = \sum_{pq} \kappa_{pq}c^\dagger_p c_q, \quad \kappa^\dagger = -\kappa.
    \end{equation}
    Moreover, we split $\kappa$ into excitations and relaxations (up and down),
    \begin{equation}
        \label{eq:agg_kappa}
        \kappa = \sum_{ai} \kappa^u_{ai}c^\dagger_a \tilde{a}_i
            + \kappa^d_{ia} c^\dagger_i \tilde{c}_a
            = \sum_{ai} \kappa^u_{ai} X_ai + \kappa^d_{ia} \tilde{X}^\dagger_{ia}.
    \end{equation} 
    
    As in any many-body formulation that includes a Lagrangian, we would like to compute 
    the first-order conditions of the Lagrangian, in order to derive what would be the 
    NOCC equation. The problem with this is that the result would be some extremely 
    lengthy expressions, because $\kappa$ does not commute with $\hat{T}$ or $\Lambda$.
    Therefore, we express the NOCC equations with an optimized basis where $\kappa=0$,
    where a solution would correspond to a stationary point of the Schrödinger equation.
    This is the same as expanding the exponentials in $\kappa$ and keeping only zero-order 
    terms. This trick leads to an algorithm which iterates between orbital transformations 
    and amplitudes until self-consistency.

    At a particular stationary point the differential of the Lagrangian
    (\autoref{eq:nocc_lagrangian}) must be zero with respect to the four sets of 
    parameters $\{\tau\}$, $\{\lambda\}$, $\{\kappa^u\}$ and $\{\kappa^d\}$, giving
    us four sets of equations,
    \begin{align}
        \frac{\partial \mathscr{L}}{\partial \lambda_{\mu_n}}
            &= \mel*{\tilde{\phi}}
            {\tilde{X}_{\mu_n} e^{-\hat{T}}\hat{H}e^{\hat{T}}}
            {\phi}, \\
        \frac{\partial \mathscr{L}}{\partial \tau_{\mu_n}}
            &= \mel*{\tilde{\phi}}
            {(1 + \Lambda)e^{-\hat{T}}[\hat{H}, X_{\mu_n}]e^{\hat{T}}}
            {\phi}, \\
        \frac{\partial \mathscr{L}}{\partial \kappa^u_{\mu_1}}
            &= \mel*{\tilde{\phi}}
            {(1 + \Lambda)e^{-\hat{T}}[\hat{H}, X_{\mu_1}]e^{\hat{T}}}
            {\phi}, \\
        \frac{\partial \mathscr{L}}{\partial \kappa^d_{\mu_1}}
            &= \mel*{\tilde{\phi}}
            {(1 + \Lambda)e^{-\hat{T}}[\hat{H}, \tilde{X}_{\mu_1}]e^{\hat{T}}}
            {\phi}.
    \end{align}

    We are now ready to outline the full algorithm of the 
    \lstinline{compute\_ground\_state()} in what we have called the \lstinline{OACCD}.
    The method is iterating over the the norm of $\kappa^u$ and $\kappa^d$, called the 
    residuals of $\kappa$, until consistency compared to a tolerance value us achieved. 
    For each such iteration, iteration over the $\tau$ and $\lambda$ double excitation
    amplitudes is performed, but at a much less strict tolarance value than under the 
    \lstinline{CoupledClusterDoubles} scheme. After the iteration over $\tau$ and $\lambda$ 
    is achieved, the values for $\kappa^u$ and $\kappa^d$ are recalculated, in order to 
    compute the aggragate $\kappa$ (\autoref{eq:agg_kappa}), 
    which in turn can be used to transform the orbitals,
    \begin{equation*}
        \begin{gathered}
            h^{(k + 1)} = e^{-\kappa} h^{(k)} e^{\kappa}, \\
            (u^{pq}_{rs})^{(k + 1)}
            = (e^{-\kappa})^p_a (e^{-\kappa})^q_b 
                (u^{ab}_{cd})^{(k)}
            (e^{\kappa})^d_s (e^{\kappa})^c_r,
        \end{gathered}
    \end{equation*}
    which (in addition to being written with incomprehensible notation) is used to compute 
    a new Fock operator. The resulting rotation of the orbitals will aid in better 
    convergence towards the ground state.

    \subsubsection{Specialised Orbital-Adaptive \lstinline{AmplitudeContainer}}

    Because of the nature of the orbital-adaptive coupled cluster scheme, it is no 
    longer sufficient to store just the amplitudes as representation of the exact 
    state. Therefore, we have implemented a subclass of the \lstinline{AmplitudeContainer}
    data structure which also contains the coefficient matrices necessary to perform the 
    required orbital transformations.

    \input{implementation/doc/oaccvector.tex}

    Like the \lstinline{AmplitudeContainer} class, this data structure also implements 
    fuctionality for addition, multiplications and iteration.
    
\subsection{Mixing of Amplitude Vectors}

    Iterative many-body methods are prone to convergence problems for certain configurations.
    This would be doubly important since we have moved to a variational description 
    of coupled cluster theory,
    where generalisations of the variational theory dictate inifitesimal variations, which 
    is not always feasible to implement.
    Moreover, an iterative optimisation scheme may not always converge properly at all. 
    Luckily, there exists numerous techniques both for controlling and acceleration 
    convergence.

    \subsubsection{Alpha mixer}

    The simplest way to ``massage'' convergence out of the coupled cluster ground state methods
    to use a dampening, where one would include a part of the result from the previous 
    iteration, here applied to the $\tau$ amplitudes,
    \begin{equation}
        \bar{\tau}^{(k+1)} = (1 - \theta)\tau^{(k+1)} + \theta\tau^{(k)},
    \end{equation}
    where $\tau^{(k+1)}$ is the current result from evaluating the amplitude equations, 
    and $\tau^{(k)}$ is the previous value. Choosing $\theta \in [0,1]$ will tune how
    much of the previous amplitude to include in the new state. The idea is to allow for 
    a more gentle transition between the iterations. We have implemented this 
    very simple mixing scheme in the \lstinline{AlphaMixer} class, which also serves as 
    a base class for further mixer implementations.

    \input{implementation/doc/mixer_alpha.tex}


    \subsubsection{The Quasi-Newton method with DIIS acceleration}

    A more sophisticated method to aid in convergence, and perhaps the most popular,
    is by performing a direct inversion of iterative subspace (DIIS). The DIIS method 
    is built to accelerate the quasi-Newton method, an we will necesarily outline the 
    quasi-Newton before we examine DIIS, which is explained in Helgaker et 
    al.\cite{helgaker2014molecular}.

    The commutator of Fock operator with the cluster operator is generally
    \begin{equation}
        \label{eq:fock_cluster_commutator}
        [\hat{f}, \hat{T}] = \sum_\mu D_\mu \tau_\mu X_\mu,
    \end{equation}
    where $\epsilon_\mu$ is the sum of unoccupied energies minus sum of all 
    occupied energies, i.e. $D^{ab}_{ij} = \epsilon_a  + \epsilon_b - \epsilon_i - \epsilon_j$,
    $\tau_\mu$ is the amplitude of a particular excitation, and $X_\mu$ is an excitation 
    operator. For CCD \autoref{eq:fock_cluster_commutator} becomes,
    \begin{equation}
        [\hat{f}, \hat{T}_2] = D^{ab}_{ij} \tau^{ab}_{ij} c^\dagger_a c^\dagger_b c_i c_j.
    \end{equation}
    This allows us to write the coupled cluster vector function $\Omega^{(0)}_\mu$,
    and its Jacobian $\Omega^{(1)}_{\mu\nu}$ of the $n$th iteration in the form 
    \begin{align}
        \label{eq:vector_function_diis}
        \Omega^{(0)}_\mu &= D_\mu \tau^{(n)}_\mu 
            + \mel{\Phi_\mu}
            {e^{-\hat{T}^{(n)}} \hat{U} e^{\hat{T}^{(n)}}}
            {\Phi_0} \\
        \label{eq:jacobian_diis}
        \Omega^{(1)}_{\mu\nu} &= D_\mu\delta_{\mu\nu} 
            + \mel{\Phi_\mu}
            {e^{-\hat{T}^{(n)}} [\hat{U}, X^\nu] e^{\hat{T}^{(n)}}}
            {\Phi_0}
    \end{align}
    which are very similar to the coupled cluster energy and amplitude equations, but 
    the matrix element contains just $\hat{U}$, the fluctuation potential, instead of 
    the entire Hamiltonian $\hat{H} = \hat{F} + \hat{U}$. HERE I AM BEING INCONSISTENT
    WITH MY NOTATION AGAIN.

    The Jacobian constists only of a diagonal part, involving differences of the 
    orbital energies, and a nondiagonal part, containing the fluctuation potential.
    The trick from \emph{Newton's} method is to expand the vector functions around 
    the set of amplitudes of the current iteration $\tau^{(n)}$,
    \begin{equation}
        \Omega(\tau^{(n)} + \Delta\tau) = \Omega^{(0)}(\tau^{(n)})
            + \Omega^{(1)}(\tau^{(n)})\Delta \tau + \dots,
    \end{equation}
    which leads to a recursion relation be neglecting terms that are nonlinear in
    $\Delta \tau$,
    \begin{equation}
        \Omega^{(1)}(\tau^{(n)})\Delta \tau^{(n)} = - \Omega^{(0)}(\tau^{(n)}).
    \end{equation}
    By inserting \autoref{eq:vector_function_diis} and \autoref{eq:jacobian_diis} 
    we get the \emph{quasi-Newton} equations for the optimisation of the 
    coupled-cluster wavefunction,
    \begin{equation}
        \label{eq:quasi_newton}
        \Delta \tau^{(n)}_\mu = - \frac{\Omega^{(0)}_\mu(\tau^{(n)})}{D_\mu}
    \end{equation}
    The quasi-Newton method is fairly robust, but the convergence may be improved 
    significantly by introducing DIIS.

    In the DIIS framework\cite{pulay1980convergence}, the new amplitudes 
    $\tau^{(n+1)}$ are obtained by a linear interpolation among the previous 
    estimates of the amplitudes,
    \begin{equation}
        \tau^{(n+1)} = \sum_{k+1}^n w_k(\tau^{(k)} + \Delta\tau^{(k)}),
    \end{equation}
    where $\Delta\tau^{(k)}$ are obtained from \autoref{eq:quasi_newton}, and 
    the interpolations weights sum to unity,
    \begin{equation*}
        \label{eq:diis_weights_sum}
        \sum_{k=1}^n w_k = 1.
    \end{equation*}
    To determine the DIIS weights, we associate each set of amplitudes $\tau^{(k)}$ 
    with an error vector. We use the scaled vector function $\Delta\tau^{(k)}$ as 
    error vector and determine the interpolation coefficients by minimising the norm of 
    the averaged vector
    \begin{equation}
        \Delta \tau^{\text{ave}} = \sum_{k=1}^n w_k \Delta \tau^{(k)}
    \end{equation}
    subject to \autoref{eq:diis_weights_sum}.

    We have implemented the DIIS acceleration of the quasi-Newton method in the class 
    \lstinline{DIIS}. This class inherits from the \lstinline{AlphaMixer} class and
    would function in its place. The \lstinline{DIIS} class allows one to pick 
    how many vectors to store and compute a linear interpolation of, with a default 
    value of $10$ vectors.

    \input{implementation/doc/diis.tex}

\section{[UNFINISHED] Time Development}

    Simlarly to the rest of the \lstinline{coupled_cluster} module, the portion relating
    to time development begins with an abstract base class,
    \lstinline{TimeDependentCoupledCluster}
    functioning as an interface for the rest of the classes. At construction, the 
    \lstinline{TimeDependentCoupledCluster} class is passed an affiliated 
    ground state solver in the form of a \lstinline{CoupledCluster} object, a 
    \lstinline{QuantumSystems} object and an \lstinline{Integrator} object. All these 
    are necessary in order to compute a time-development. The starting point of a 
    time development is a system at it's ground state, necessitating the specification 
    of a system and a ground state solver. The system is developed in time by solving 
    the equations of motion with a numerical integrator. We will consider 
    integrators separately in the next section.

    Inclusion of a \lstinline{CoupledCluster} object in the 
    \lstinline{TimeDependentCoupledCluster} class allows one to call the 
    \lstinline{compute_ground_state()} from this object, and it is included as a 
    wrapper. Severak other methods are included from the ground state realm, like 
    the methods for particle density computations. 

    The bare minimum that a time-dependent coupled cluster scheme needs to implement 
    in order to function is the methods \lstinline{rhs_t_amplitudes()} and 
    \lstinline{rhs_l_amplitudes()}, which should return the right-hand side of 
    the amplitude equations. These methods should be integrated as generators, to make 
    it possible to iterate over them, and should yield the amplitudes in order of 
    increasing excitation level.

    \input{implementation/doc/tdcc.tex}

    Arguably the most important method in the \lstinline{TimeDependentCoupledCluster}
    abstract base class is the \lstinline{solve(time_steps)} method. For the array 
    of time steps supplied, this method propagates with the integrator member of the 
    class for all amplitudes. This method remains the same for all time-propagation 
    schemes, and is therefore implemented in the base class for inheritance in 
    sub-classes. This method in full is
    \begin{python}
    def solve(self, time_points, timestep_tol=1e-8):
        n = len(time_points)

        for i in range(n - 1):
            dt = time_points[i + 1] - time_points[i]
            amp_vec = self.integrator.step(
                self._amplitudes.asarray(), time_points[i], dt
            )

            self._amplitudes = type(self._amplitudes).from_array(
                self._amplitudes, amp_vec
            )

            if abs(self.last_timestep - (time_points[i] + dt)) > timestep_tol:
                self.update_hamiltonian(time_points[i] + dt, self._amplitudes)
                self.last_timestep = time_points[i] + dt

            yield self._amplitudes
    \end{python}
    We see that after the integrator is advanced one step in time, returning an amplitude 
    vector. This amplitude object is stored as a member of the class by use of the 
    \lstinline{from_array()} method from the \lstinline{AmplitudeContainer} class,
    after which the  
    Hamiltonian of the system is updated if enough time has passed.

    \noindent -------- \\
    SECTION ON IMPLEMENTATION OF \lstinline{__call__} HERE? \\
    \noindent -------- 

    \subsection{TDCCSD}
    We have implemented both a time-dependent CCD (TDCCD) solver and a time-dependent CCSD
    (TDCCSD)
    solver. For the sake of brevity, we present only the TDCCSD here. 
    The \lstinline{TDCCSD} class, a sub-class of \lstinline{TimeDependentCoupledCluster},
    inheriting all methods from this super-class. It accepts the same parameter as the super-class, except the 
    parameter that defines the ground state solver to be used - the \lstinline{CoupledCluster}
    class implementation. The ground state solver is already decided by the level of 
    excitation for the computation at hand. All parameters are passed to the constructor 
    in the parent class.

    The \lstinline{solve()} method will have the exact same functionality as in the parent class,
    but since the \lstinline{TDCCSD} contains amplitudes and everything else needed to 
    solve the equations of motions in a singles and doubles trunctation, it will now yield a 
    \lstinline{Generator} object 
    containing amplitudes that are developed in time. Any observable can be extracted during 
    an iteration over this \lstinline{Generator} object. We have implemeted several methods that 
    can be useful in extracting information about the state of the time-developed system,
    for instance \lstinline{compute_time_dependent_overlap} which computes the 
    probability of the system being in the ground state, and \lstinline{compute_energy} 
    which computes the energy of the system in the current time-dependent state.
    The ground state probability, i.e. \lstinline{compute_time_dependent_overlap}, is 
    given by a general time-dependent auto-correlation function,
    \begin{equation}
        \label{eq:td_autocorr_1}
        A(t', t) \equiv \braket{S(t')}{S(t)}.
    \end{equation}
    Because coupled cluster theory is not variational in the usual sense it is necessary to 
    define a general state vector as combination of both $\ket{\Psi}$ and $\bra*{\tilde{\Psi}}$,
    \begin{equation}
        \ket{S} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            \ket{\Psi} \\ \ket*{\tilde{\Psi}}
        \end{pmatrix}
    \end{equation}
    which makes the time-dependent auto-correlation function (\autoref{eq:td_autocorr_1}),
    \begin{equation}
        A(t', t) = \frac{1}{2} 
        \left( \braket*{\tilde{\Psi}(t')}{\Psi(t)} 
            +  \braket*{\Psi(t')}{\tilde{\Psi}(t)}  \right)
    \end{equation}
    according to the definitions of the \emph{indefinite} innerproduct by
    \citeauthor{pedersen2019symplectic}\cite{pedersen2019symplectic}. 
    Here we would set $t'=0$, because we are
    interested in the ground state overlap, translating to the state before developement 
    in time.

    \input{implementation/doc/tdccsd.tex}

    Within our truncation to includ only single- and double escitations,
    an inner product of two state vectors, in the normal coupled cluster scheme
    with static orbitals, can be computed in the following manner
    \begin{equation}
        \begin{aligned}
            \braket{\Psi'}{\Psi} 
            =& \mel{\Phi}
            {
                (1 + \Lambda)e^{-\hat{T}'} e^{\hat{T}}
            }
            {\Phi} \\
            =&
            \mel{\Phi}{
            (1 + \Lambda_1 + \Lambda_2)
            (1 - \hat{T}'_1 - \hat{T}'_2 + \frac{1}{2}\hat{T}'^2_1) 
            (1 + \hat{T}_1 + \hat{T}_2 + \frac{1}{2}\hat{T}^2_1)
            }{\Phi} \\
            =& \braket{\Phi} - \mel{\Phi}{\Lambda_1\hat{T}'_1}{\Phi} 
                + \mel{\Phi}{\Lambda_1\hat{T}'_1}{\Phi}
                - \mel{\Phi}{\Lambda_2\hat{T}'_1\hat{T}_1}{\Phi}
                - \mel{\Phi}{\Lambda_2\hat{T}'_2}{\Phi} \\
            \ & + \mel{\Phi}{\Lambda_2\hat{T}_2}{\Phi}
                + \frac{1}{2}\mel{\Phi}{\Lambda_2\hat{T}'_1\hat{T}'_1}{\Phi}
                + \frac{1}{2}\mel{\Phi}{\Lambda_2\hat{T}_1\hat{T}_1}{\Phi},
        \end{aligned}
    \end{equation}
    where we have ignored terms that would give a zero-contribution.
    Evaluating the remainding terms can be done with your 
    favourite method. Here is an example using Wick's theorem,
    \begin{equation}
        \begin{aligned}
            \mel{\Phi}{\Lambda_2\hat{T}_2}{\Phi} 
            &= \mel{\Phi} 
            {
            \sum_{abij} \frac{1}{4}\lambda^{ij}_{ab}
                \{\hat{i}^\dagger \hat{a} \hat{j}^\dagger \hat{b} \}
            \sum_{cdkl} \frac{1}{4}\tau^{cd}_{kl}
                \{\hat{c}^\dagger \hat{k} \hat{d}^\dagger \hat{l} \}           
            } 
            {\Phi} \\
            &= \mel{\Phi} 
            {
            \sum_{\substack{abcd\\ijkl}} \frac{1}{16}\lambda^{ij}_{ab} \tau^{cd}_{kl}
                \wick{
                \{\c1{\hat{i}^\dagger} \c2{\hat{a}} \c3{\hat{j}^\dagger} \c4{\hat{b}} \}
                \{\c2{\hat{c}^\dagger} \c1{\hat{k}} \c4{\hat{d}^\dagger} \c3{\hat{l}} \}
                } 
            } 
            {\Phi}  + \text{three more equivalent contractions} \\
            &= 
            \frac{1}{4} \mel{\Phi} 
            {
                \sum_{\substack{abcd \\ ijkl}} \lambda^{ij}_{ab} \tau^{cd}_{kl} 
                \delta_{ac} 
                \delta_{bd}
                \delta_{ik} 
                \delta_{jl}
            } {\Phi}
            = \frac{1}{4} \sum_{abij} \lambda^{ij}_{ab} \tau^{ab}_{ij}.
        \end{aligned}
    \end{equation}

    The entirity of the \lstinline{compute_time_dependent_overlap_method()} consists of
    similar computations,
    \begin{python}
    def compute_time_dependent_overlap():
        np = self.np
        t_0, t_1, t_2, l_1, l_2 = self._amplitudes.unpack()
        t_1_0, t_2_0 = self.cc.t_1, self.cc.t_2 
        l_1_0, l_2_0 = self.cc.l_1, self.cc.l_2

        psi_t_0 = 1
        psi_t_0 += np.einsum("ia, ai ->", l_1, t_1_0)
        psi_t_0 -= np.einsum("ia, ai ->", l_1, t_1)
        psi_t_0 += 0.25 * np.einsum("ijab, abij ->", l_2, t_2_0)
        psi_t_0 -= 0.5 * np.einsum("ijab, aj, bi ->", l_2, t_1_0, t_1_0)
        psi_t_0 -= np.einsum("ijab, ai, bj ->", l_2, t_1, t_1_0)
        psi_t_0 -= 0.5 * np.einsum("ijab, aj, bi ->", l_2, t_1, t_1)
        psi_t_0 -= 0.25 * np.einsum("ijab, abij ->", l_2, t_2)
    
        psi_0_t = 1
        psi_0_t += np.einsum("ia, ai ->", l_1_0, t_1)
        psi_0_t -= np.einsum("ia, ai ->", l_1_0, t_1_0)
        psi_0_t += 0.25 * np.einsum("ijab, abij ->", l_2_0, t_2)
        psi_0_t -= 0.5 * np.einsum("ijab, aj, bi ->", l_2_0, t_1_0, t_1_0)
        psi_0_t -= np.einsum("ijab, ai, bj ->", l_2_0, t_1, t_1_0)
        psi_0_t -= 0.5 * np.einsum("ijab, aj, bi ->", l_2_0, t_1, t_1)
        psi_0_t -= 0.25 * np.einsum("ijab, abij ->", l_2_0, t_2_0)
    
        auto_corr = 0.5 * (psi_t_0 * np.exp(-t_0) + (psi_0_t * np.exp(t_0)).conj())

        return np.abs(auto_corr) ** 2
    \end{python}

    \subsection{OATDCCD}

    \paragraph{Disappearing RHS of Q-space equations.}

    A necessary additon to an orbital-adaptive time-dependent coupled cluster framework 
    is the computation of $P$- and $Q$-space equations. The $Q$-space equations can 
    be simplified substatially, because they equate to zero for an infinite basis. 
    We will show this now, starting with \autoref{eq:oatdccd_q_1},
    \begin{equation}
        i\hbar\sum_q \rho^q_p Q \frac{\partial}{\partial t} \ket{\varphi_q} 
        = \sum_q \rho^q_p Q h\ket{\varphi_q}
        + \sum_{qrs} \rho^{qs}_{pr} Q W^r_s \ket{\varphi_q}.
    \end{equation}
    Inserting for $Q$ in the second term on the right-hand side gives
    \begin{equation}
        \sum_{qrs} \rho^{qs}_{pr} Q W^r_s \ket{\varphi_q} 
        = \sum_{qrs} \rho^{qs}_{pr} W^r_s\ket{\varphi_q} 
        - \sum_{qrs} \rho^{qs}_{pr} W^r_s
            \sum_t \ket{\varphi_t}\braket{\tilde{\varphi}_t}{\varphi_q}.
    \end{equation}
    If we assume an infinite orthogonal basis, we have 
    \begin{equation*}
        \sum_t \dyad{\varphi_t}{\tilde{\varphi}_t}\ket{\varphi_q} = \hat{1},
    \end{equation*}
    and the term will disappear. Inserting for $Q$ in the first term on the 
    right hand side of the first $Q$-space equations also yields zero. This 
    means that the first $Q$-space equations reduce to 
    \begin{equation}
        \begin{aligned}
        i\hbar \sum_q \rho^q_p Q \frac{\partial}{\partial t} &= 0 \\
        i\hbar \sum_q \rho^q_p \frac{\partial}{\partial t} \ket{\varphi_p}
        &=
        i\hbar \sum_q \rho^q_p
            \sum_s \dyad{\varphi_s}{\tilde{\varphi}_s}\frac{\partial}{\partial t}
            \ket{\varphi_p} \\
        \frac{\partial}{\partial t}\ket{\varphi_p(t)}
        &=
        \sum_s \ket{\varphi_s(t)}
            \mel*{\tilde{\varphi}_s(t)}{\frac{\partial}{\partial t}}{\varphi_p(t)} \\
        \frac{\partial}{\partial t} C^\alpha_p(t) \ket{\chi_\alpha}
        &=
        \sum_s C^\alpha_s(t) \ket{\chi_\alpha} \eta^s_p \\
        \dot{C}^\alpha_p &= \sum_s C^\alpha_s \eta^s_p,
        \end{aligned}
    \end{equation}
    which we rewrite more nicely on einsten summation form,
    \begin{equation}
        \dot{\vb{C}} = \vb{C}\eta^p_q.
    \end{equation}
    Similarly for the second $Q$-space equations (\autoref{eq:oatdccd_q_2}),
    \begin{equation}
       \dot{\tilde{\vb{C}}} = -\eta^p_q\tilde{\vb{C}}.
    \end{equation}
    We see that the $Q$ space equation has provided us with equations that 
    describe the time propagation of the orbitals through the coefficient 
    matrices $\vb{C}$ and $\tilde{\vb{C}}$. These equations are valid 
    for all excitations levels of orbital-adaptive time-dependent coupled cluster
    (OATDCC), and have been implemented 
    in the new abstract class \lstinline{OATDCC}.

    \input{implementation/doc/oatdccd.tex}

\subsection{Integrators and ODE Solvers}

    Most, if not all, physical systems that evolve in time will can be described by 
    as set of equations that we call the equations of motion. These can be usually 
    be formulated as a single- or a set of ordinary differential equations written 
    on the abstract form
    \begin{equation}
        \label{eq:general_ode}
        u'(t) = f(u(t), t).
    \end{equation}
    To this particular equation there is an infinite number of solutions, so in order to
    make the solution unique, we must also specify an initial condition
    \begin{equation}
        u(0) = U_0.
    \end{equation}
    Given the right hand side of \autoref{eq:general_ode}, $f(u,t)$ and the initial
    condition $U_0$, our task would be to compute $u(t)$. The simplest equation of motion
    in physics is Newton's second law,
    \begin{equation}
        a(t) = \frac{F(t)}{m},
    \end{equation}
    which we have reformulated to be on the standard form as \autoref{eq:general_ode}.
 
    In any numerical scheme, the ODE defining our problem will be discretised, such 
    that the inital value problem \autoref{eq:general_ode} becomes the following 
    \begin{equation}
        u_{n+1} = u_n + hf(u_n, t_n), \quad u(t_0) = u_0,
    \end{equation}
    where $h$ is some small time step, $t_{n+1} = t_n + h$. We see that the equation(s) at 
    hand is solve in steps and the most important method of an implementation of any integrator 
    scheme will be the method defining how one would step from one point to the next. 
 
    We have already derived the equations of motions for several coupled 
    cluster frameworks\footnote{TDCC:\autoref{eq:tdcc_tau} and
    \autoref{eq:tdcc_lambda}.
    OATDCC:\autoref{eq:oatdcc_tau}, \autoref{eq:oatdcc_lambda}}. Solving these equations 
    in time is done in the same manner as any other equations of motion. The right hand 
    side of these equations is put into practice by implementing \lstinline{__call__()} for 
    all the time-dependent classes, and the initial condition of the problem is some 
    configuration defined by the amplitudes of the problem. By formulating the time-dependent
    many-body problem in this way, we can find solutions to the equations of motion by 
    any numerical integrator scheme. For convenience, we have included two integrator 
    implementations in the \lstinline{coupled_cluster} module - the common fourth order 
    Runge-Kutta method and the symplectic Gauss-Legendre method. Moreover, we have defined an 
    abstract base class \lstinline{Integrator}, which defines a general integrator for 
    eventual future additons.

    \input{implementation/doc/integrator.tex}

    \subsubsection{The Runge-Kutta Method}

    The Runge-Kutta methods are a large family of implicit and iterative methods or 
    increasing order. The first-order Runge-Kutta method is the same as the forward 
    Euler method, where a step is defined as follows, 
    \begin{equation}
        u_{n+1} = u_n + h f(u_n, t_n).
    \end{equation}
    The general step of an explicit $n$-th order Runge-Kutta method is defined
    by
    \begin{equation}
        u_{n+1} = u_n = h \sum_{i=1}^s b_i k_i,
    \end{equation}
    where 
    \begin{equation}
        \begin{aligned}
            k_1 =& f(u_n, t_n), \\
            k_2 =& f(u_n + h(a_{21}k_1), t_n + c_2h), \\
            k_3 =& f(u_n + h(a_{31}k_1 + a_{32}k_2), t_n + c_3h), \\
                \vdots& \\
            k_s =& f(u_n + h(a_{s1}k_1 + a_{s2}k_2 + \dots + a_{s,s-1}k_{s-1}), t_n + c_s),
        \end{aligned}
    \end{equation}
    where $s$ is the number of stages, and the coefficients $a_{ij}$ 
    (for $j\in[1,i\rangle$ and $i\in\langle j, s]$), $b_i$ (for $b\in[1,s]$)
    and $c_i$ (for $i\in[2,s]$) defines the particular method. The matrix 
    $a_{ij}$ is called the Runge-Kutta matrix and $b_i$ and $c_i$ are known as the 
    \emph{weights} and \emph{nodes}, respectively. We call the Runge-Kutta method consistent 
    if
    \begin{equation*}
        \sum_{j=1}^{i-1} a_{ij} = c_i, \quad i \in[2,s].
    \end{equation*}

    We have implemented the fourth-order Runge-Kutta method in the class
    \lstinline{RungeKutta4}. This is the most common of the Runge-Kutta method, and is 
    often sometimes referred to as simply ``the Runge-Kutta method''. 

    \input{implementation/doc/RK4.tex}

    A step of size $h$ in the fourth order Runge-Kutta method is defined by 
    \begin{equation}
       \begin{aligned}
            u_{n+1} =& u_n + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4), \\
            t_{n+1} =& t_n + h,
       \end{aligned} 
    \end{equation} 
    where
    \begin{align*}
        k_1 =& hf(u_n, t_n), \\
        k_2 =& hf(u_n + \frac{k_1}{2}, t_n + \frac{h}{2}), \\
        k_3 =& hf(u_n + \frac{k_2}{2}, t_n + \frac{h}{2}), \\
        k_4 =& hf(u_n + k_3, t_n + h),
    \end{align*}
    This is implemented in the \lstinline{step(u, t, dt)} method as
    \begin{python}
    f = self.rhs
    K1 = dt * f(u, t)
    K2 = dt * f(u + 0.5 * K1, t + 0.5 * dt)
    K3 = dt * f(u + 0.5 * K2, t + 0.5 * dt)
    K4 = dt * f(u + K3, t + dt)
    u_new = u + (1 / 6.0) * (K1 + 2 * K2 + 2 * K3 + K4 
    \end{python}

    \subsubsection{Symplectic Gauss Integrator}

    The Runge-Kutta method, as described above, will be unstable for most systems
    becuase of its inability to preserve structure and energy of the system. It is 
    necessary to apply an integrator which is both structure-preserving and 
    symplectic.
    We have inherited code used by 
    \citeauthor{pedersen2019symplectic}\cite{pedersen2019symplectic} and have 
    adapted it to our framework. Nevertheless, we give a brief overview of 
    its inner mechanics here.

    A quadrature rule is an approximation of the definite integral of a function over 
    an interval $[a,b]$.

    The most common family of quadrature rules are derived by 
    defining an equidistant grid of $N$ points on the interval $[a,b]$,
    where the grid points $x_n$ are given by 
    \begin{equation}
        x_n = a + nh
    \end{equation}
    where $h = (b - a)/N$, with index $n\in[0, N]$. A quadrature rule is commonly stated 
    as a weighted sum of function values at specified points. 
    \begin{equation*}
        \int_a^b f(x) dx \approx \sum_{i=1}^{(N-1)} h f(x_i).
    \end{equation*}
    The simplest of such schemes of equidistant points is the \emph{trapezoidal} rule 
    given by 
    \begin{equation}
        \int_a^b f(x) dx = 
            h \left(\frac{1}{2}g(x_0) + f(x_1) + f(x_2) + \dots 
                f(x_{N-1}) + \frac{1}{2}f(x_N) \right) + \mathscr{O}(h^2).
    \end{equation}

    A very efficient method consists of repeating the trapezoidal rule and 
    performing it for successive values of $h$, each having half the size of the 
    previous one. This yields a sequence of approximations to the integral for various 
    values of $h$ can be fitted to a polynomial, and the value for this polynomial 
    for $h=0$ will yield a very accurate approximation to the exact value. This is
    called the \emph{Romberg} method.

    The $n$-point Gaussian quadrature rule functions similarly to the family of methods 
    described above, but instead of equidistant points we use the zeros of 
    orthogonal polynomials for the gripd points $x_n$. The first pick of orthogonal 
    polynomials are Legendre polynomials, which are orthogonal on the interval $[-1,1]$,
    i.e.,
    \begin{equation}
        \int_{-1}^1 P_l(x) P_{l'}(x) dx = \delta_{ll'}.
    \end{equation}
    We also approximate the function $f$ with Legendre polynomials. 
    
    The Gauss-Legendre quadrature rule is constructed to yield an exact 
    result for polynomials of degree $2n-1$ or less. An advantage of the Gauss-Legendre 
    method is that its accuracy is much better than that of other methods using the 
    same number of integration points. In fact, the accuracy of an $N$-point Gauss-Legendre 
    method is equivalent to that of an equidistiant point method using $2N$ points.
    The resulting Gauss-Legendre quadrature rule can be stated as 
    \begin{equation}
        \int_{-1}^1 f(x) dx = \sum_{n=1}^N w_n f(x_n) + \mathscr{O}(h^{2N}),
    \end{equation}
    where $x_n$ are the zeroes of the Legendre polynomial $P_n$, $h$ is $2/N$ and 
    $w_n$ are appropriately chosen weights for the method.
    
    Orthogonal polynomials $p_r$ of degree $r$ and 
    leading coefficient one, satisfy the following recurrence realtion,
    \begin{equation}
        P_{r+1}(x) = (x - a_{r,r})p_r(x) - a_{r,r-1}p_{r-1}(x) \dots -a_{r,0}p_0(x).
    \end{equation}
    The trhee-term recurrence relation can be written as a matrix equation
    \begin{equation}
        J\tilde{P} = x\tilde{P} - p_n(x) \times \vb{e}_n,
    \end{equation}
    where $\tilde{P} = [p_0(x),p_1(x),\dots,p_{n-1}(x)]^T$, $\vb{e}_n$ is the $n$th 
    standard basis vector and $J$ is the Jacobian matrix,
    \begin{equation}
        J = \begin{pmatrix}
            a_0 &    1  &    0  & \dots   &         & \\
            b_1 &  a_1  &    1  &    0    & \dots   & \\
            0   &  b_2  &   a_2 &    1    &    0    & \dots\\
            0   & \dots &       &         & \dots   & 0 \\
                & \dots &   0   & b_{N-2} & a_{N-2} & 1 \\
                &       & \dots &      0  & n_{N-1} & a_{N-1} \\
        \end{pmatrix}
    \end{equation}

    The eigenvalues of this matrix will be the nodes $x_n$, i.e. the zeros of the 
    polynomials up to degree $N$. If $\phi^{(n)}$ is an eigenvector corresponding to 
    an eigenvalue such an eigenvalue $x_n$, the corresponding weight can be found 
    from the first component of this vector
    \begin{equation}
        w_n = \mu_0 \left(\phi_1^{(n)} \right)^2,
    \end{equation}
    where 
    \begin{equation*}
        \mu_0 = \int_a^b \omega(x) dx
    \end{equation*}
    and $\omega(x)$ is the weight function. $\omega(x) = 1$ when Legendre polynomials
    are used in the Gauss quadrature. This efficient way of arriving at weights and 
    nodes is called the Golub-Welsh algorithm\cite{golub1969calculation}.
   
    Generally, a quadrature method is not used to compute the solution to ODEs, but we 
    adapt it to a Runge-Kutta solver in the way explained in 
    \citeauthor{pedersen2019symplectic}\cite{pedersen2019symplectic}. A general 
    implicit $s$-stage Runge-Kutta method is defined by 
    \begin{align}
        u_{n+1} = u_n + h\sum_{i=1}^s x_i f(u_n + Z_{ij}, t_n + w_i h), \\
        \label{eq:z_solve}
        Z_{in} h\sum_{j=1}^s a_{ij} f(u_n + Z_{jn}, t_n + w_j h).
    \end{align}
    This allows us to make an interpolation between each time step $t_n$ and $t_n + h$
    by a polynomial of order $s$ and requiring the ODE to be satisfied at the $s$
    Gauss-Legendre quadrature points gives a symplectic and reversible integrator of
    order $2s$. The matrix $a_{ij}$ is computed analytically,
    \begin{equation}
        a_{ij} = \int_0^{w_j} \ell_j(x) dx,
    \end{equation}
    where
    \begin{equation}
        \ell_j(x) = \prod_{k=1,k\neq j}^s \frac{x - w_k}{w_j - w_k},
    \end{equation}
    is the $j$th Lagrange interpolation polynomial.
    The nonlinear equation \autoref{eq:z_solve} is solved iteratively for each time
    step, making the method implicit. These fixed-point iterations are defined by 
    \begin{equation}
        Z^{(k+1)}_{in} = h\sum_{j=1}^s a_{ij}f(u_n + Z^{(k)}_{jn}, t_n + w_j h).
    \end{equation}
    The initial guess is crucial to the convergence speed of the method. We have 
    employed guess (A) scheme described in section VIII.6.1 of
    Ref.\cite{hairer2006geometric}.

    For the user of the Gauss integrator, the experience will be much more pleasant 
    than dealing with the derivations of the method, because its opeartion
    are the same, as evidenced by the \lstinline{GaussIntegrator}
    class specification. 

    \input{implementation/doc/gauss_integrator.tex}